{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Machine learning crash course notes Week - 1  (Enes Çavuş) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bu Notebook Crash Course  tensorflow api egzersizleri içermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Colab original link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=linear_regression_synthetic_tf2-colab&hl=en#scrollTo=QF0BFRXTOeR3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of teaching should not be to help the students learn how to memorize and spit out information under academic pressure. The purpose of teaching is to inspire the desire for learning in them and make them able to THINK, UNDERSTAND and  QUESTION. \n",
    "\n",
    " **- Richard Feynman**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model eğitminde kullanılan hiperparametrelerin optimum değerlerinin bulunması."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bence bilgisayar bilimlerinin temelinde **( divide and conquer ) _böl ve yönet_** ilkesi yatar. Eğer çok büyük bir projeyi ekip çalışması yapmadan, planlayıp parçalamadan (Bakınız - kitap - [**Scrum - Agile**](https://www.amazon.com.tr/Scrum-Kat%C4%B1-Zamanda-Yapma-Sanat%C4%B1/dp/6058487471/ref=asc_df_6058487471/?tag=googleshoptr-21&linkCode=df0&hvadid=344882438061&hvpos=&hvnetw=g&hvrand=4972712707744752112&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9056855&hvtargid=pla-713402970464&psc=1) ), iş bölümü yapmadan sizce başarıyla tamamlama olasılığınız nedir? Yoktur. Bunu her alanda uygulayabiliriz ama şimdilik bu ilk haftanın konuları üzerine yoğunlaşalım.\n",
    "\n",
    "Tabii ki tüm konular çok önemli ama,\n",
    "Ben bu haftayı dörde böldüm:\n",
    "+ Feature ve label kavramlarını anlama\n",
    "+ Loss yönetimine hakimiyeti artırma\n",
    "+ Laerning rate, batch, epoch hiperparametrelerini anlama\n",
    "+ Veriyi yönetme ve overfitting gibi  olası problemlerden kaçınma ( Train - Validation - Test ) Bu kısma bu notebook üzerinde değinilmemiştir.\n",
    "\n",
    "\n",
    "**Note:** Bu yazdıklarım genel hatırlatma amaçlıdır. Arada girip göz atabileceğiniz CheatSheet. Detaya inmeden sadece genel bir bakış.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this Colab on TensorFlow 2.x    # bu kısımda colab üzerinde kullanacağımız tensorflow sürümü seçini yapılmaktadır. Pek bi önemi yok şu anda\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                           # pandas veriyi analiz edebilmek için vazgeçilmez bir kütüphane. Bence ilk 10 da belki de 5\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt          # matplotlib veriyi görselleştirmede öncü, bilmeyenler için seaborn'u da araştırabilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sıradaki kodumuz model tanımlama işlemlerini içermektedir ve şu anda odağımızda değildir. Geçiniz! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential. \n",
    "  # A sequential model contains one or more layers.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer. \n",
    "  model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that \n",
    "  # TensorFlow can efficiently execute. Configure \n",
    "  # training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model           \n",
    "\n",
    "\n",
    "def train_model(model, feature, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Feed the feature values and the label values to the \n",
    "  # model. The model will train for the specified number \n",
    "  # of epochs, gradually learning how the feature values\n",
    "  # relate to the label values. \n",
    "  history = model.fit(x=feature,\n",
    "                      y=label,\n",
    "                      batch_size=None,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Gather the history (a snapshot) of each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # Specifically gather the model's root mean \n",
    "  #squared error at each epoch. \n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return trained_weight, trained_bias, epochs, rmse\n",
    "\n",
    "print(\"Defined create_model and train_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aşağıdaki kod, görselleştirme araçlarınn kulanılmasını sağlamakta ve yine bununla şimdilik işimiz yok. Evet veri görselleştirme ve veriyi analiz etme makine öğrenmesinin temelleri, ilk yapılması gerekenler ama şimdilik odak noktamız daha çok model eğitiminin teorik temelleri ve belli başlı parametrelere aşinalık kazanmamız. Bu yüzden bu kodu da geçelim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(\"feature\")\n",
    "  plt.ylabel(\"label\")\n",
    "\n",
    "  # Plot the feature values vs. label values.\n",
    "  plt.scatter(feature, label)\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = my_feature[-1]\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimizde bir veri olmadan model eğitemeyeceğimize göre. Bizler için güzelce hazırlanmış, tam bir regresyon problemine uygun verimizi ve label değerlerini aşağıda görmekteyiz. Yeri gelmişken label ve feature kavramlarını biraz daha yakından inceleyelim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature\n",
    " - Bizler modelimizi eğitirken ona bilgi vermekteyiz. Şu özellikleri al ve bunlardan bi çıkarım yap. Örneğin kurstaki ev örneğini alalım. Bi evin özellikleri neler olabilir : Kaç odalı? Kaç metrekare? Kaç banyosu var? Göl manzaralı mı? (böyle bir özellik olmayabilir de :) ). İşte bunlar bizim modelimize vereceğimiz özellikler olacak. Peki bunları nasıl yorumlayacak bu model? İşte burada label devreye giriyor.\n",
    "    \n",
    "## Label\n",
    " - Şu cümleye geri dönelim ->  _Şu özellikleri al ve bunlardan bir çıkarım yap_. Nasıl bir çıkarım yapacağını söyledik mi? Hayır. Bunu söylemek için label kullanıyoruz. Ekteki resime bakıldığında buradaki feature ve label değerleri görülmekte. Biz modelimize oda sayılarını vb veriyoruz ayrıca ek olarak ona bu özelliklere sahip bi evin değeri şu olur diyerek label'lama işlemini de yapmış oluyoruz. Oldukça anlaşılır dimi? Yine de bir örnek verelim.\n",
    "\n",
    "## Örnek\n",
    " Bir evimiz var ve bunun 4 odası, 2 banyosu, bahçesi var ve 150 metrekare ( bunlar özelliklerimizdi ) , değeri ise $260.000 (bu label).\n",
    " Bir evimiz var ve bunun 3 odası, 1 banyosu, bahçe yok, ve 120 metrekare (bunlar özelliklerimizdi ), değeri ise #130.000 (bu label)\n",
    " \n",
    " Bu bilgilerden milyonlarca olduğunu düşünün. Sizce bir evin bahçesi varsa değerinin artacağını modelimiz öğrenemez mi? Ya da banyo sayısının artması ile fiyatın arasında bir oran orantı yakalayamaz mı?\n",
    "\n",
    "\n",
    "![test image](images/feature-label.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
    "my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verimizi aldığımıza göre modelimizin hiperparametrelerine bi göz atalım.\n",
    "\n",
    "+ Learning_rate : learning rate kısaca verimiz üzerinde nasıl gezineceğimizi belirler. Verimizi yorumlarken düşük bir loss değerine erişmek iyidir fakat learning rate i çok küçük alırsanız ömür boyu erişemeyebilirsiniz. Ya da çok büyük alırsanız hedefimizin üzerinden atlar ve çok farklı konumlara şıçrayabilirsiniz.\n",
    " \n",
    "+ Batch_size: gerçek hayatta veri setlerimiz çok büyük ve bunu bir kerede modele verip yorumlamasını istememiz hatta bunu sürekli yaptırmamız belki de imkansız ya da zaman alır. Bunu önlemek için modelimize parça parça veri veriyoruz. Örneği: 1000 veriniz var batch size ise 100 seçtiniz, işte bu şekilde 10 tura tamamen verimiz modelden geçmiş olacaktır.Tur demişken ...\n",
    "\n",
    "+ Epoch : Tur dediğimiz şey aslında iterasyon sayısı. Örneğimizde bu 10 du. Peki bu 10 turun ( iterasyonun ) tamamlanması için geçen zamana ne deniyor? Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "epochs=10\n",
    "my_batch_size=12\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kısımda epoch hparametresini yorumlamamız istenmiş. Artırabiliriz, Aşırıya kaçmayalım çünkü overfitting hiç istemediğimiz bişey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "epochs= ?   # Replace ? with an integer.\n",
    "my_batch_size=12\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                        my_label, epochs,\n",
    "                                                        my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kısımda learning rate'i yorumlamamız istenmiş. Yukarıda da açıklamıştım: Ne çok yüksek ne de çok düşük değerler vermeyelim, aşırıya kaçmadan orta yolu bulmamız kâfi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the learning rate and decrease the number of epochs.\n",
    "learning_rate=100 \n",
    "epochs=500 \n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kısımda ikisinide yorumlamamız istenmiş. Eğer learning rate küçülürse epoch'un artması işimize yarayabilir. Ya da tam tersi: learning rate büyürse epoch azalsa iyi olur.\n",
    "\n",
    "Optimumu bulmak için sürekli denemeler yapmamız gerekebilir. Ve kötü yanı, hiçbir zaman net bi değer yoktur. Bu parametreler veri setlerine özgü belirlenmeli. En garanti yol bu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and number of epochs\n",
    "learning_rate= ?  # Replace ? with a floating-point number\n",
    "epochs= ?   # Replace ? with an integer\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu örnekte batch_size'ın öneminin anlaşılamayacağını düşünüyorum. Daha büyük veri setlerinde farklı kombinasyonlarda görüşmek üzere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.05\n",
    "epochs=100\n",
    "my_batch_size=12  # Replace ? with an integer.\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                        my_label, epochs,\n",
    "                                                        my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
