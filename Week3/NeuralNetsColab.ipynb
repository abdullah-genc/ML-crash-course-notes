{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks [ Colab Egzersizi](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/intro_to_neural_nets.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=intro_to_nn_tf2-colab&hl=en#scrollTo=Pu7R_ZpDopIj)\n",
    "\n",
    "Not: En verimli sonuç için colab üzerinde çalıştırınız. Lokal üzerinde denerseniz import veya sürüm hataları alabilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bu colab egzersizinde, daha önceden yapmış olduğumuz model eğitim aşamalarını bu kez nöral ağlar kullanarak yapmaya çalışacağız. Basit bir derin öğrenme ağı oluşturacak ve hiperparemetreleri üzerinde iyileştirmeler yapacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Her zamanki gibi gerekli import işlemlerini yapıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diğer uygulamlarda olduğu gibi burada da \"house dataset\" i kullanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the examples\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizasyon\n",
    "\n",
    "Burada Z-skoru elde ediyoruz.  Peki Z - Score neydi?\n",
    "\n",
    "**[(verimiz - verimizin ortalaması ) / standart sapma ]**\n",
    "\n",
    "Amacımız özelliklerimiz arasındaki dengesizlikleri gidermek. Örneğin bir özelliğimizin değerlerinin aralığı **100 - 200000** olsun bir diğeri **10 - 30** olsun. Bu problemleri ortadan kaldırıyoruz.\n",
    "\n",
    "[Daha fazlası icin](https://developers.google.com/machine-learning/data-prep/transform/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Convert raw values to their Z-scores \n",
    "\n",
    "# Calculate the Z-scores of each column in the training set:\n",
    "train_df_mean = train_df.mean()\n",
    "train_df_std = train_df.std()\n",
    "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
    "\n",
    "# Calculate the Z-scores of each column in the test set.\n",
    "test_df_mean = test_df.mean()\n",
    "test_df_std = test_df.std()\n",
    "test_df_norm = (test_df - test_df_mean)/test_df_std\n",
    "\n",
    "print(\"Normalized the values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature tanımlama işlemleri\n",
    "\n",
    "Dikkat etmemiz gereken yerler.\n",
    "\n",
    "Binning ve feature cross yaptığımız kodlar. \n",
    "\n",
    "Bir önceki uygulamalarımızda da vardı zaten. Yeni bir şey yok.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "# We scaled all the columns, including latitude and longitude, into their\n",
    "# Z scores. So, instead of picking a resolution in degrees, we're going\n",
    "# to use resolution_in_Zs.  A resolution_in_Zs of 1 corresponds to \n",
    "# a full standard deviation. \n",
    "resolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n",
    "\n",
    "# Create a bucket feature column for latitude.\n",
    "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude_boundaries = list(np.arange(int(min(train_df_norm['latitude'])), \n",
    "                                     int(max(train_df_norm['latitude'])), \n",
    "                                     resolution_in_Zs))\n",
    "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n",
    "\n",
    "# Create a bucket feature column for longitude.\n",
    "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
    "longitude_boundaries = list(np.arange(int(min(train_df_norm['longitude'])), \n",
    "                                      int(max(train_df_norm['longitude'])), \n",
    "                                      resolution_in_Zs))\n",
    "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n",
    "                                                longitude_boundaries)\n",
    "\n",
    "# Create a feature cross of latitude and longitude.\n",
    "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\n",
    "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
    "feature_columns.append(crossed_feature)  \n",
    "\n",
    "# Represent median_income as a floating-point value.\n",
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "feature_columns.append(median_income)\n",
    "\n",
    "# Represent population as a floating-point value.\n",
    "population = tf.feature_column.numeric_column(\"population\")\n",
    "feature_columns.append(population)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting, sonuçlarımızı görselleştirmek için."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function.\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "  plt.show()  \n",
    "\n",
    "print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standart linear model tanımlamalarımız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define functions to create and train a linear regression model\n",
    "def create_model(my_learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Add one linear layer to the model to yield a simple linear regressor.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model           \n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, label_name):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "\n",
    "  # Get details that will be useful for plotting the loss curve.\n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse   \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear model için tuning işlemleri ve eğitim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 1000\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df_norm, epochs, batch_size, label_name)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derin öğrenme ağı tasarlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asıl konumuz olan derin ağ modeli kısmına geldik.\n",
    "\n",
    "Üç önemli madde var:\n",
    "- Saklı katmanlar  (Hidden layer)\n",
    "- Aktivasyon Fonksiyonu (ReLU)\n",
    "- Output katmanları\n",
    "\n",
    "Kod üzerinde yorumlar ile anlatmaya çalıştım...\n",
    "\n",
    "![nn](images/nn3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate, my_feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(my_feature_layer)\n",
    "\n",
    "\n",
    "\n",
    "  # Dense layer nedir? Dense layer is the regular deeply connected neural network layer. It is most common and frequently used layer. Dense layer does the below operation on the input and return the output.\n",
    "  # Resimdeki sarı düğümlerden oluşan inputtan sonra gelen katmanımız. Ama burada 3 değil 20 node var ve aktivasyon fonksiyonmuzda görüldüğü üzere ReLU seçilmiş.      \n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Bu kez ise yine bir gizli katman ama 12 düğümle. Aktivasyon fonksiyonumuz yine ReLU.\n",
    "  model.add(tf.keras.layers.Dense(units=12, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden2'))\n",
    "  \n",
    "  # Define the output layer. Bu örnekte sadece 1 outputumuz olduğundan ( median_house_value ), 1 tane output düğümümüz var\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buradan sonra bilgiğimiz model train aşamaları ve hiperparametreler üzerinde yenilemeler yapıyoruz.\n",
    "\n",
    "### Bu konuyu daha iyi anlamak için **katman ekleme** , **düğüm sayılarını değiştirme** gibi pratikler yapabilir ve **loss** değerlerine göz gezdirebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "\n",
    "# Specify the label\n",
    "label_name = \"median_house_value\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set. We're passing the entire\n",
    "# normalized training set, but the model will only use the features\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_df_norm, epochs, \n",
    "                          label_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ek kaynaklar\n",
    "- [Course link](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/video-lecture)\n",
    "- [Colab Link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/intro_to_neural_nets.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=intro_to_nn_tf2-colab&hl=en#scrollTo=hI7ojsL7nnBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Enes Çavuş_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
